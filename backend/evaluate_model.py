"""
evaluate_model.py
-----------------
Runs TF Object Detection API eval loop and summarizes COCO-style metrics (mAP).
USAGE:
    python backend/evaluate_model.py \
        --pipeline_config tf_od/pipeline.config \
        --model_dir tf_od/models/exp \
        --checkpoint <ckpt-number-or-path> \
        --out_dir tf_od/eval_reports

NOTES:
- Relies on TF OD API's evaluation tooling (object_detection.model_main_tf2).
- Expects TFRecords and label_map generated by train_model.py.
- After eval run, you can parse eval metrics produced in model_dir/events or metrics JSON.
"""
import argparse
import os
import subprocess
from pathlib import Path
import json
import glob

def run_eval(pipeline_config, model_dir, checkpoint=None, run_once=True):
    cmd = [
        "python",
        "-m", "object_detection.model_main_tf2",
        f"--pipeline_config_path={pipeline_config}",
        f"--model_dir={model_dir}",
        "--checkpoint_dir=" + str(model_dir),
        "--alsologtostderr"
    ]
    if run_once:
        cmd.append("--eval_timeout=30")  # let eval run once (30s timeout)
    print("Running evaluation:")
    print(" ".join(cmd))
    subprocess.run(cmd, check=True)

def find_metrics_json(model_dir):
    # TF OD may write metrics to model_dir or to eval subfolders
    paths = list(Path(model_dir).rglob("*eval*"))
    # Example: metrics may be in events or in metrics.json depending on setup
    # We'll try to find *.tfrecord or metric files
    jsons = list(Path(model_dir).rglob("*.json"))
    return [str(p) for p in jsons]

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--pipeline_config", required=True)
    parser.add_argument("--model_dir", required=True)
    parser.add_argument("--checkpoint", default=None)
    parser.add_argument("--out_dir", default="tf_od/eval_reports")
    args = parser.parse_args()

    pipeline_config = Path(args.pipeline_config)
    model_dir = Path(args.model_dir)
    out_dir = Path(args.out_dir)
    out_dir.mkdir(parents=True, exist_ok=True)

    # Run TF OD API eval loop (requires TF OD API available)
    run_eval(str(pipeline_config), str(model_dir), args.checkpoint)

    # Try to find any JSON-summary metrics and copy to out_dir
    found = find_metrics_json(model_dir)
    for f in found:
        try:
            shutil.copy(f, out_dir / Path(f).name)
        except:
            pass

    print("Evaluation completed. Check TensorBoard logs in model_dir for detailed metrics (mAP, precision, recall).")
    print(f"Copied candidate JSON metric files: {found}")
